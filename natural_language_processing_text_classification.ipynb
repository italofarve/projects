{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/italofarve/projects/blob/main/natural_language_processing_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamiento de lenguaje natural aplicado a la clasificación de mensajes de 7 categorías distintas. \n",
        "\n",
        "\n",
        "El modelo implementado fue un SVM con un 0.9849 de precisión. Se uso scikit-learn, alternativamente se podría usar las herramientas de Google o AWS. \n",
        "\n",
        "\n",
        "A continuación se explican:\n",
        "\n",
        "- Los detalles del proyecto\n",
        "- El desarrollo de la solución\n",
        "- Se compara la performance del modelo SVM con otros modelos \n",
        "\n",
        "\n",
        "Contenido del proyecto\n",
        "\n",
        "\n",
        "![Contenido del proyecto](https://github.com/italofarve/projects/blob/main/nlp-text-classification.png?raw=true)\n",
        "\n",
        "\n",
        "# Detalles de la misión\n",
        "Nuestro departamento de inteligencia ha clasificado todas las comunicaciones interceptadas\n",
        "durante las últimas semanas en 7 categorías distintas, atendiendo a las divisiones Rebeldes a las\n",
        "que pertenecen. Su misión es conseguir que el sistema de clasificación sea capaz de clasificar una\n",
        "nueva transmisión de manera automática para que el departamento de operaciones pueda evaluar\n",
        "las amenazas más rápido.\n",
        "\n",
        "El resultado de esta misión deben ser 2 ejecutables: * train: Recibirá como único argumento el\n",
        "nombre de una carpeta de entrada. Esta carpeta contendrá una subcarpeta por cada categoría,\n",
        "dentro de las cuales estarán los ejemplos. A partir de esa carpeta, train entrenará un modelo y\n",
        "generará un archivo \"model\" con el modelo entrenado. Ejemplo de uso:"
      ],
      "metadata": {
        "id": "AELfap5s6K6e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdmYg1sgQlTs"
      },
      "source": [
        "# Pasos seguidos en la solución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skke5HXFQlTy"
      },
      "source": [
        "## 1. Observación de los datos\n",
        "\n",
        "En primer lugar, teniendo en cuenta que era un problema de clasificación de textos, se empezo a bucear dentro de las carpetas observando las características del contenido de los archivo de forma manual.\n",
        "\n",
        "**Se observo:**\n",
        "\n",
        "- Es un problema de clasificación similar al de clasificación de textos o  imágenes, cuyas muestras están ordenadas por carpetas (categorías). Tendremos en cuenta todos los caracteres por si los rebeldes enviaron mensajes secretos.\n",
        "\n",
        "- Los archivos con un formato de nombre \".!31339!252524\" estaban ocultos al explorador de archivos. Una manipulación manual de división de dataset podría provocar copiar aquellos archivos en los conjuntos de entrenamiento y test. Se verifico que los datos no se repitieran utilizando linea de comandos $ ls -l\"\n",
        "\n",
        "\n",
        "- Los archivos no contenian datos o metadatos como: from, subjetc, pero sí emails, firmas que de cierto modo representan las relaciones entre individuos y junto con el tipo de contenido que se comparte generán patrones que muestran una relación entre indivuos y/o fortalecen la categorización. Por ejemplo: escritores de contenido económico para diferentes medios pueden compartir estilo de prosa y colaborar entre ellos más que escritores de distintos tipos de contenido. Seguimos pensando en trabajar con todos los caracteres.\n",
        "\n",
        "\n",
        "- Se observa documentos con distinta cantidad de caracteres por lo que necesitaremos de técnicas para compensarlo. Nos decidimos por usar Scikit-learn para empezar a manipular los datos por dos motivos: podemos aprovecharnos de su arquitectura y optimización de procesos (he seguido la misma estrategia para analizar imágenes, usando primero scikit-learn y luego añadiendo OpenCV. En este caso podría ser Scikit-learn + NLTK/TensorFlow o herramientas cloud de Google / AWS). \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GJmfV6GL6J3w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FAOd7oEQlT1"
      },
      "source": [
        "## 2. Cargando datos para seguir analizandolos\n",
        "\n",
        "- Previamente hicimos una carga de datos para visualizar todos datos en su conjunto, sin embargo aprovecharemos esta fase para seguir observando los datos.\n",
        "\n",
        "- **división de datos:** Hemos extraido 19 elementos de cada categoría para que nos sirva de test en este primer ensayo. El 20 % de 400 es 20 y en cada categoria tenemos muestras que rondan los 400 - 600 muestras: \n",
        "\n",
        "--- carpeta de datos de entrenamiento: dataset\n",
        "\n",
        "--- carpeta de datos de test: test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEL4M2zIQlT2"
      },
      "outputs": [],
      "source": [
        "# Importamos el modulo datasets para empezar a manipular los datos\n",
        "import sklearn.datasets as skds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtxJfVkmQlT5"
      },
      "outputs": [],
      "source": [
        "# Carpeta de nuestros datos de entrenamiento \n",
        "path_train = 'dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbCYvUCYQlT5"
      },
      "source": [
        "- load_files nos ayuda a cargar los datos de una carpeta (path_train) y carga los datos en memoria (load_content=True), barajamos los datos como antes de repartir barajas (shuffle=True) y añadimos una semilla para poder replicar el experimento (random_state=42).\n",
        "- Es importante añadir un \"encoding\" de lo contrario se cargaran los archivos como \"bytes\" y no podremos utilizar otras funciones para procesar texto como las de extracción de características: CountVectorizer, TfidfTransformer, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbt3dEmzQlT7"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos\n",
        "files_train = skds.load_files(path_train, load_content=True, shuffle=True, encoding='latin1', random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqH96E3_QlT8",
        "outputId": "823129bc-fd05-49ff-c657-82421dea6fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({4: 590, 5: 577, 1: 575, 0: 574, 6: 527, 3: 471, 2: 446})\n"
          ]
        }
      ],
      "source": [
        "# Contamos los datos que tenemos de cada categoria\n",
        "import collections\n",
        "print(collections.Counter(files_train.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLxqWlGYQlT-",
        "outputId": "8881f2a9-4f2c-4728-bd27-62517ce178d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3760"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Cantidad total de archivos para el entrenamiento\n",
        "len(files_train.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVioY_0dQlT_"
      },
      "source": [
        "- Observamos que efectivamente hay un número diferente de archivos por cada categoría pero scikit-learn nos da funciones para poder compensar de este desbalance. Lo veremos más adelante.\n",
        "\n",
        "- Apuntamos el orden de las categorias, nos servirá para crear el archivo \"classify.py\". De esta forma aprovecharemos la estructura de carpetas que tiene scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThyBFqMyQlUA",
        "outputId": "089c766f-0e87-4a88-8277-f7a822707076"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "files_train.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK98TIdlQlUB",
        "outputId": "ebdabbba-8e03-4aa0-84fe-1e1135a4a8b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['exploration',\n",
              " 'headhunters',\n",
              " 'intelligence',\n",
              " 'logistics',\n",
              " 'politics',\n",
              " 'transportation',\n",
              " 'weapons']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualizamos orden de categorias\n",
        "files_train.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGQgjrCzQlUC"
      },
      "source": [
        "- Con las siguientes dos instrucciones podemos visualizar el contenido de un archivo y su categoría. Observamos que un ser humano incorporado al sistema de retroalimentación del modelo sería de mucha utilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0-x2V4fQlUF",
        "outputId": "5562ba3e-550a-4854-bd14-b65a474e3062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article-I.D.: cactus.1993Apr6.060553.22453\n",
            "\n",
            "In article <YfkBJQS00Uh_E9TFo_@andrew.cmu.edu> \"Daniel U. Holbrook\" <dh3q+@andrew.cmu.edu> writes:\n",
            ">>>\n",
            ">\n",
            "[stuff about RHD deSoto's deleted]\n",
            "\n",
            ">Well Sweden and Australia, and lord knows wherever else used to drive on\n",
            "Australians still do drive on the \"wrong\" side of the road. I believe\n",
            "Sweden changed in 1968. The way I heard it was that they swapped\n",
            "all the traffic signs around one Sunday....\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Visualizando contenidos de distintos archivos cambiando \".data[numero].\"\n",
        "# Observamos que hablan de como conducen los autralianos y podemos deducir la categoría \n",
        "# \"transportation\"\n",
        "\n",
        "print(\"\\n\".join(files_train.data[9].split(\"\\n\")[:12]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm1odRNKQlUH",
        "outputId": "f48566b1-07eb-44d6-f928-25a727df1553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transportation\n"
          ]
        }
      ],
      "source": [
        "print(files_train.target_names[files_train.target[9]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOBbwhcBQlUI"
      },
      "source": [
        "### 3. Extracción de características de los archivos de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssYf_q6zQlUJ"
      },
      "source": [
        "CountVectorizer incluye el preprocesamiento del texto, la tokenización y el filtrado de las palabras, como resultado construye un diccionario de caracteristicas y los documentos quedan convertidos en vectores de caracteristicas con los que ya podemos trabajar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWkx0SiaQlUL",
        "outputId": "3acd592a-cb7e-479f-d49b-7a921a5ceb2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3760, 46585)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(files_train.data)\n",
        "X_train_counts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW-jYOhTQlUM",
        "outputId": "049e482f-71f1-4992-fb56-867b52408d2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7779"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Podemos saber las veces que aparece la palabra algorithm en nuestro vocabulario.\n",
        "count_vect.vocabulary_.get(u'algorithm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl7DdthyQlUN"
      },
      "source": [
        "**¿Por qué es importante contar cuantas veces aparece una palabra?**\n",
        "\n",
        "Es un primer paso hacia la predicción. Si aparece mucho la palabra \"conducir\" y \"viajes\", probablemente el texto encaje en la categoría \"transportation\"\n",
        "\n",
        "**Limites del conteo de palabras**\n",
        "\n",
        "La frecuencia que aparece una palabra puede ser mayor en textos más grandes y menor en textos más cortos aunque se refieran al mismo tema, por lo que observamos un problema. \n",
        "\n",
        "Para resolver esto se resuelve dividiendo el número de veces que aparece una palabra en un documento por el número total de palabras del documento **(Term Frequencies)**. Adicionalmente se aplica una reducción de los pesos de las palabras que aparecen en muchos documentos que son menos informativas **(“Term Frequency times Inverse Document Frequency”)**\n",
        "\n",
        "\n",
        "Aplicando ambas transformaciones: Term Frequencies y Term Frequency times Inverse Document Frequency (**TfidfTransformer**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEi5A2Z9QlUO",
        "outputId": "a9810816-fce4-4146-fd3d-bfe176e3371e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3760, 46585)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j0EOGuvQlUP"
      },
      "source": [
        "## 4. Probando clasificador Naive Bayes\n",
        "\n",
        "Empezamos probando el Naive Bayes. Observar que tenemos que usar el mismo proceso de transformación de texto realizado con los datos de entrenamiento.\n",
        "\n",
        "El Naive Bayes Multinomial es una de las variantes usadas en clasificación de texto. La distrubución está parametrizada por vectores para cada clase y se trabaja con la probabilidad de que una caracteristica \"i\" aparezca en una muestra perteneciente a una clase \"y\". Por defecto el suavizado (\"alfa\" = 1) que también se conoce como Laplace smoothing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yvOpomoQlUQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, files_train.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIXqZvjSQlUR",
        "outputId": "42a544a5-29d2-4025-d975-8174728e74c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'one-day Navy Scientific Visualization and Virtual Reality Seminar. The purpose of the seminar is to present and exchange information for Navy-related scientific visualization and virtual reality programs, research, developments, and applications.' => headhunters\n",
            "': Colin Greenwood from Scotland Yard did a study that showed that gun : control has had no effect on crime or murder rates in the UK.  His book,: _Firearms_Controls_, has been published in London by Keegan Paul (name : may be misspelled).' => weapons\n"
          ]
        }
      ],
      "source": [
        "docs_new = [\"one-day Navy Scientific Visualization and Virtual Reality Seminar. The purpose of the seminar is to present and exchange information for Navy-related scientific visualization and virtual reality programs, research, developments, and applications.\", \n",
        "            \": Colin Greenwood from Scotland Yard did a study that showed that gun : control has had no effect on crime or murder rates in the UK.  His book,: _Firearms_Controls_, has been published in London by Keegan Paul (name : may be misspelled).\"]\n",
        "\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "\n",
        "# Entrenamos el modelo\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "predicted = clf.predict(X_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "            print('%r => %s' % (doc, files_train.target_names[category]))\n",
        "            \n",
        "           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1lmtvhPQlUS"
      },
      "source": [
        "## 5. ¿Por qué construir un PipeLine?\n",
        "\n",
        "\n",
        "Hasta ahora hemos aplicado el vectorizado, y el tratamiento de ocurrencias a frecuencias tanto para los datos de entrenamiento como para test. Sin embargo Scikit-learn nos permite automatizar todos esas fases en una sentencia de código. Esto nos permitirá probar nuevos módelos de forma más ágil y es lo que usaremos en el archivo \"train.py\" para crear nuestro modelo de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEHjEt8VQlUT"
      },
      "outputs": [],
      "source": [
        "## Construyendo un pipeline de vectorizado, tf-idf y empleando el Naive Bayes Multinomial\n",
        "\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUNvw_YDQlUU",
        "outputId": "11774f36-1677-43b6-fabd-636b805bc1c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf', MultinomialNB())])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Entrenamos el modelo\n",
        "text_clf.fit(files_train.data, files_train.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee1gJ9IfQlUV"
      },
      "source": [
        "## ¿Dónde están nuestras muestras de test?\n",
        "\n",
        "Hemos guardado los archivos de test en la carpeta llamada \"test\". Usar la misma estrategia y estructura que nos proporciona scikit-learn para cargar datos nos proporcionará agilidad para probar \"n\" modelos diferentes. Ver el notebook adjuntado llamado \"otros-modelos.ypnb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVRbBh_CQlUW"
      },
      "outputs": [],
      "source": [
        "path_test = 'test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AHkJns1QlUX",
        "outputId": "33042478-0417-4c28-d8c9-34721d213402"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# files_test = skds.load_files(path_test,load_content=False)\n",
        "files_test = skds.load_files(path_test, load_content=True, shuffle=True, encoding='latin1', random_state=42)\n",
        "files_test.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_xyrV7LQlUX",
        "outputId": "a37733ae-f94f-4988-f8eb-24bfa750d0a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "133"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observamos que 133 = 19 * 7, por lo que no se nos ha colando archivos ocultos.\n",
        "\n",
        "len(files_test.filenames)\n",
        "# files_test.keys()\n",
        "# files_test.filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT4fyfP6QlUY"
      },
      "source": [
        "## 6. Evaluamos performance del clasificador Naive Bayes Multinomial\n",
        "\n",
        "\n",
        "### Precisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZDsdAKUQlUZ",
        "outputId": "ce20a4c5-8c72-47ce-eeab-42ea0a3fb189"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.924812030075188"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Accuracy de 92.48%\n",
        "\n",
        "import numpy as np\n",
        "docs_test = files_test.data\n",
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == files_test.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU1xdJaGQlUb"
      },
      "source": [
        "### Reporte más completo del modelo\n",
        "\n",
        "Recordemos que el recall es la tasa de verdaderos positivos, muestras que han sido correctamente identificadas\n",
        "\n",
        "Observemos el **recall** para la categoría **intelligence** es de 0.53. Del reporte de métricas y la matriz de confusión vemos que el clasificador está confundiendo casi la mitad de muestras con la categoría **weapons**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkVREdE-QlUf",
        "outputId": "964d0402-cf36-469b-e50a-9531c3b733ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "   exploration       1.00      1.00      1.00        19\n",
            "   headhunters       1.00      0.95      0.97        19\n",
            "  intelligence       1.00      0.53      0.69        19\n",
            "     logistics       1.00      1.00      1.00        19\n",
            "      politics       0.95      1.00      0.97        19\n",
            "transportation       1.00      1.00      1.00        19\n",
            "       weapons       0.68      1.00      0.81        19\n",
            "\n",
            "      accuracy                           0.92       133\n",
            "     macro avg       0.95      0.92      0.92       133\n",
            "  weighted avg       0.95      0.92      0.92       133\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(\n",
        "    files_test.target, \n",
        "    predicted,\n",
        "    target_names=files_test.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5jCcBiAQlUg",
        "outputId": "7756e7d2-4148-44a3-94e7-7e009ebe4046"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[19,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 18,  0,  0,  1,  0,  0],\n",
              "       [ 0,  0, 10,  0,  0,  0,  9],\n",
              "       [ 0,  0,  0, 19,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 19,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0, 19,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0, 19]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics.confusion_matrix(files_test.target, predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQhGVZZJQlUh"
      },
      "source": [
        "## 7. Probando el clasificador SVM\n",
        "\n",
        "Una de las ventajas de emplear un SVM es que son eficases cuando el número de dimensiones de las muestras es muy grande (recordemos que nuestras muestras han sido vectorizadas). Por ejemplo SVM para dos clases trabaja apoyandose en dos vectores de soporte, uno por cada clase, que marcan las fronteras de clasificación. Imaginemos la frontera de dos paises con una zona neutral de igual distancia para ambas partes. Los comienzos de cada zona neutral estarán marcados por lineas o hiperplanos, la distancia perpendicular entre ambas lineas se conoce como margen, este margen podría ser máximo o minimo. Las funciones de perdida se pueden considerar como un limite superior del error de clasificación.\n",
        "\n",
        "- loss (hace referencia a soft-margin)\n",
        "- penalty (hace referecia al entorno de regulación)\n",
        "- alpha (constante, cuanto más alto es el valor, más fuerte es la regularización)\n",
        "- max_iter (número de épocas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF4bbuRIQlUi",
        "outputId": "a80101b1-842e-4336-dcf4-110877460c02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('clf',\n",
              "                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,\n",
              "                               tol=None))])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Usando SVM\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
        "                          alpha=1e-3, random_state=42,\n",
        "                          max_iter=5, tol=None)),\n",
        "])\n",
        "\n",
        "text_clf.fit(files_train.data, files_train.target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvEruioLQlUi"
      },
      "source": [
        "## 8. Evaluamos el performance del nuevo modelo SVM\n",
        "\n",
        "### Precisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAJ-u3byQlUj",
        "outputId": "4cea5577-a3af-455d-be8d-fd3de0e24612"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9849624060150376"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Empleando clasificador\n",
        "predicted = text_clf.predict(docs_test)\n",
        "\n",
        "# calculando precisión\n",
        "np.mean(predicted == files_test.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w36EjeD6QlUl"
      },
      "source": [
        "### Reporte más completo\n",
        "\n",
        "Recordemos que el recall es la tasa de verdaderos positivos, muestra que han sido correctamente identificadas\n",
        "\n",
        "Observemos el **recall** para la categoría **intelligence** pasa de 0.53 a 0.95. Del reporte de métricas y la matriz de confusión vemos que el clasificador está confundiendo sólo 2 muestras. Este modelo es más optimo y procederemos a implementarlo para la entrega.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQCV0zlWQlUm",
        "outputId": "03da0604-1083-4ff3-b1f0-f02858b98e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "   exploration       1.00      1.00      1.00        19\n",
            "   headhunters       0.95      1.00      0.97        19\n",
            "  intelligence       1.00      0.95      0.97        19\n",
            "     logistics       1.00      1.00      1.00        19\n",
            "      politics       1.00      1.00      1.00        19\n",
            "transportation       0.95      1.00      0.97        19\n",
            "       weapons       1.00      0.95      0.97        19\n",
            "\n",
            "      accuracy                           0.98       133\n",
            "     macro avg       0.99      0.98      0.98       133\n",
            "  weighted avg       0.99      0.98      0.98       133\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(\n",
        "    files_test.target, \n",
        "    predicted,\n",
        "    target_names=files_test.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPHHcZU8QlUn",
        "outputId": "f20f963d-71e9-4761-ee64-880d720197d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[19,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 19,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0, 18,  0,  0,  1,  0],\n",
              "       [ 0,  0,  0, 19,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0, 19,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0, 19,  0],\n",
              "       [ 0,  1,  0,  0,  0,  0, 18]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics.confusion_matrix(files_test.target, predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIKkvQqYQlUo"
      },
      "source": [
        "## 9. Conclusiones y recomendaciones\n",
        "\n",
        "- Mantenemos las 19 muestras para test por cada categoría y el analisis de todos los caracteres. El proceso de extracción de características nos permite ganar ágilidad pero si la cantidad de datos de entrenamiento se multiplica habría que trabajar en la eliminación de algunas palabras.\n",
        "\n",
        "- Para implementar elegimos el modelo basado en SVM con un 0.9849 de precisión despues de hacer pruebas con otros algoritmos que no superaban el perfomance del SVM (ver el notebook \"otros-modelos.ipynb\").\n",
        "\n",
        "- De las diferencias en las matrices de confusión concluimos la necesidad de utilizar, adicionalmente, alguno de los algoritmos que superaban el 98% de precisión para una próxima implementación o implementación A/B, esto nos permitirá balancear los errores de clasificación y detectar cambios en las transmisiones.\n",
        "\n",
        "- También recomendamos establecer una sistema de votaciones con 5 algoritmos de precisión mayor de 95%, estableciendo un threshold de probabilidad que permita incluir la intervención de un humano dentro del ciclo de mejora del sistema de clasificación de textos. Las costumbres y conexiones humanas cambian, eso se podría reflejar en los documentos, y el sistema debería adaptarse de manera continua. \n",
        "\n",
        "- Scikit-learn nos permite guardar modelos haciendo uso de la liberia \"joblib\" y es la que hemos utilizado al no poder crear archivos ejecutables. Recomendamos su uso ya que su peso es muchisimo más liviano y son más seguros. Un archivo ejecutable siempre podría contener codigo malicioso.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjs8FQvpQlUp"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "natural language processing text classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}